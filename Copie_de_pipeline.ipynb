{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1Dzpy4S-Y3L",
        "outputId": "6ab683f8-8411-4e5c-e22f-1bb1927ae89c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting paho-mqtt\n",
            "  Downloading paho_mqtt-2.1.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading paho_mqtt-2.1.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m931.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, paho-mqtt, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5 paho-mqtt-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow numpy pandas paho-mqtt keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "1SyE6vmZBK-B",
        "outputId": "9c835afa-74b1-41e9-bd03-de7eac8804dc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-98889628-58fc-48bf-8eee-a589b195c18f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-98889628-58fc-48bf-8eee-a589b195c18f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving renault_can_data.xlsx to renault_can_data.xlsx\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, LSTM, GRU, Conv1D, MaxPooling1D, UpSampling1D,\n",
        "                                    RepeatVector, Dense, TimeDistributed, Dropout,\n",
        "                                    BatchNormalization, Concatenate, Add)\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import keras_tuner as kt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "import os\n",
        "import json\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Global variables\n",
        "X_test_global = None\n",
        "timesteps_global = 20\n",
        "n_features_global = 0\n",
        "\n",
        "def convert_numpy_types(obj):\n",
        "    \"\"\"\n",
        "    Convert numpy types to native Python types to avoid serialization issues\n",
        "    \"\"\"\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_numpy_types(item) for item in obj]\n",
        "    elif pd.isna(obj):\n",
        "        return None\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "def safe_float_conversion(value):\n",
        "    \"\"\"\n",
        "    Safely convert numpy types to float, handling NaN and infinity\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if pd.isna(value):\n",
        "            return np.nan\n",
        "        elif isinstance(value, (np.floating, np.integer)):\n",
        "            float_val = float(value)\n",
        "            if np.isinf(float_val):\n",
        "                return np.nan\n",
        "            return float_val\n",
        "        elif isinstance(value, (int, float)):\n",
        "            if np.isinf(value):\n",
        "                return np.nan\n",
        "            return float(value)\n",
        "        else:\n",
        "            return np.nan\n",
        "    except (ValueError, TypeError, OverflowError):\n",
        "        return np.nan\n",
        "\n",
        "def decode_payload(row):\n",
        "    \"\"\"\n",
        "    Decode CAN payload with proper error handling and numpy type conversion\n",
        "    \"\"\"\n",
        "    try:\n",
        "        payload = str(row['Payload']).replace(' ', '')\n",
        "        # Handle cases where payload might be NaN or invalid\n",
        "        if pd.isna(payload) or payload == 'nan':\n",
        "            return pd.Series({})\n",
        "\n",
        "        # Ensure payload has even length for proper byte parsing\n",
        "        if len(payload) % 2 != 0:\n",
        "            payload = '0' + payload\n",
        "\n",
        "        bytes_data = [int(payload[i:i+2], 16) for i in range(0, len(payload), 2)]\n",
        "        can_id = str(row['ID'])\n",
        "        decoded = {}\n",
        "\n",
        "        if can_id == '18FF10E5' and len(bytes_data) >= 1:\n",
        "            decoded['Vehicle_Speed_kmh'] = float(bytes_data[0])  # Convert to native float\n",
        "\n",
        "        elif can_id == '18FF50E5' and len(bytes_data) >= 6:\n",
        "            decoded['BMS_Level'] = float(bytes_data[0])\n",
        "            decoded['BMS_Voltage_V'] = float((bytes_data[2] << 8 | bytes_data[3]) * 0.1)\n",
        "            decoded['BMS_Current_A'] = float(bytes_data[4] << 8 | bytes_data[5])\n",
        "\n",
        "        elif can_id == '18FF21E5' and len(bytes_data) >= 1:\n",
        "            decoded['Motor_Power_kW'] = float(bytes_data[0])\n",
        "\n",
        "        elif can_id == '18FF31E5' and len(bytes_data) >= 1:\n",
        "            decoded['Temperature_C'] = float(bytes_data[0])\n",
        "\n",
        "        elif can_id == '18FF40E5' and len(bytes_data) >= 8:\n",
        "            decoded['Charger_Status'] = float(bytes_data[0])\n",
        "            decoded['alpha'] = float((bytes_data[1] << 8 | bytes_data[2]) * 0.01)\n",
        "            decoded['betha'] = float((bytes_data[3] << 8 | bytes_data[4]) * 0.01)\n",
        "            decoded['gamma'] = float(bytes_data[5] << 8 | bytes_data[6])\n",
        "            decoded['khi'] = float(bytes_data[7])\n",
        "\n",
        "        return pd.Series(decoded)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding row: {e}\")\n",
        "        return pd.Series({})\n",
        "\n",
        "def compute_acceleration(df):\n",
        "    \"\"\"Compute acceleration from vehicle speed with numpy type handling\"\"\"\n",
        "    # Create a copy to avoid warnings\n",
        "    df = df.copy()\n",
        "\n",
        "    # Convert speed to m/s with safe conversion\n",
        "    df['speed_m_s'] = df['Vehicle_Speed_kmh'].fillna(0).astype(float) / 3.6\n",
        "\n",
        "    # Calculate time differences\n",
        "    df['delta_time'] = df['Timestamp'].diff().dt.total_seconds().astype(float)\n",
        "\n",
        "    # Calculate speed differences\n",
        "    df['delta_speed'] = df['speed_m_s'].diff().astype(float)\n",
        "\n",
        "    # Calculate acceleration with proper handling of division by zero\n",
        "    df['acceleration'] = np.where(\n",
        "        (df['delta_time'] > 0) & (df['delta_time'].notna()),\n",
        "        df['delta_speed'] / df['delta_time'],\n",
        "        0.0\n",
        "    ).astype(float)\n",
        "\n",
        "    return df\n",
        "\n",
        "def check_data_quality(df):\n",
        "    \"\"\"Check data quality and report issues with numpy type handling\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DATA QUALITY REPORT\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Convert shapes to native Python types\n",
        "    shape_info = (int(df.shape[0]), int(df.shape[1]))\n",
        "    memory_usage = float(df.memory_usage(deep=True).sum() / 1024**2)\n",
        "\n",
        "    print(f\"Dataset shape: {shape_info}\")\n",
        "    print(f\"Memory usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "    duplicates = int(df.duplicated().sum())\n",
        "    print(f\"Duplicate rows: {duplicates}\")\n",
        "\n",
        "    missing = df.isnull().sum()\n",
        "    if int(missing.sum()) > 0:\n",
        "        print(f\"\\nMissing values:\")\n",
        "        for col, count in missing[missing > 0].items():\n",
        "            print(f\"  {col}: {int(count)}\")\n",
        "    else:\n",
        "        print(\"\\nNo missing values found.\")\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        infinite_vals = np.isinf(df[numeric_cols]).sum()\n",
        "        if int(infinite_vals.sum()) > 0:\n",
        "            print(f\"\\nInfinite values:\")\n",
        "            for col, count in infinite_vals[infinite_vals > 0].items():\n",
        "                print(f\"  {col}: {int(count)}\")\n",
        "        else:\n",
        "            print(\"\\nNo infinite values found.\")\n",
        "\n",
        "        print(f\"\\nNumeric data ranges:\")\n",
        "        # Convert describe output to avoid numpy types\n",
        "        desc_stats = df[numeric_cols].describe()\n",
        "        for col in desc_stats.columns:\n",
        "            print(f\"{col}:\")\n",
        "            for stat in desc_stats.index:\n",
        "                value = safe_float_conversion(desc_stats.loc[stat, col])\n",
        "                print(f\"  {stat}: {value:.6f}\" if not np.isnan(value) else f\"  {stat}: NaN\")\n",
        "\n",
        "def prepare_can_data():\n",
        "    \"\"\"Prepare and process CAN data with numpy type handling\"\"\"\n",
        "    print(\"PREPARING CAN DATA\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        df = pd.read_excel('renault_can_data.xlsx')\n",
        "        df.to_csv('data.csv', index=False, encoding='utf-8')\n",
        "        print(\"Excel file converted to CSV\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Warning: renault_can_data.xlsx not found, trying to read existing CSV...\")\n",
        "        try:\n",
        "            df = pd.read_csv('data.csv', parse_dates=['Timestamp'])\n",
        "        except FileNotFoundError:\n",
        "            print(\"Error: No data file found. Please provide renault_can_data.xlsx\")\n",
        "            return None, None, None\n",
        "\n",
        "    print(\"Decoding CAN payloads...\")\n",
        "    decoded_df = df.apply(decode_payload, axis=1)\n",
        "\n",
        "    result_df = pd.concat([df, decoded_df], axis=1)\n",
        "    result_df.to_csv('decoded_can_data.csv', index=False)\n",
        "    print(\" Decoding complete. Output saved to 'decoded_can_data.csv'\")\n",
        "\n",
        "    print(\"Reading decoded data...\")\n",
        "    data = pd.read_csv(\"decoded_can_data.csv\", parse_dates=['Timestamp'])\n",
        "    print(f\" Data shape: {data.shape}\")\n",
        "\n",
        "    print(\"Creating one-hot encoding for CAN IDs...\")\n",
        "    one_hot_encoded = pd.get_dummies(data['ID'], prefix='ID').astype(float)  # Ensure float type\n",
        "    data = pd.concat([data, one_hot_encoded], axis=1)\n",
        "\n",
        "    print(\"Computing acceleration...\")\n",
        "    data = compute_acceleration(data)\n",
        "    data['acceleration'].fillna(0.0, inplace=True)\n",
        "    data['acceleration'] = data['acceleration'].astype(float)\n",
        "\n",
        "    features = [\n",
        "        'BMS_Current_A', 'BMS_Level', 'BMS_Voltage_V', 'Charger_Status',\n",
        "        'Motor_Power_kW', 'Temperature_C', 'Vehicle_Speed_kmh','ID_18FF10E5','ID_18FF21E5','ID_18FF50E5','ID_18FF31E5','ID_18FF40E5','acceleration']\n",
        "    existing_features = [f for f in features if f in data.columns]\n",
        "    missing_features = [f for f in features if f not in data.columns]\n",
        "\n",
        "    print(f\" Existing features: {existing_features}\")\n",
        "    if missing_features:\n",
        "        print(f\" Missing features: {missing_features}\")\n",
        "\n",
        "    if existing_features:\n",
        "        print(\"Interpolating missing values...\")\n",
        "        df_features = data[existing_features].copy()\n",
        "\n",
        "        # Convert columns to numeric with explicit float64 type\n",
        "        for col in existing_features:\n",
        "            df_features[col] = pd.to_numeric(df_features[col], errors='coerce').astype(np.float64)\n",
        "\n",
        "        df_features = df_features.interpolate(method='linear', axis=0)\n",
        "\n",
        "        # Fill any remaining NaN values with 0 *before* scaling\n",
        "        df_features = df_features.fillna(0.0).astype(np.float64)\n",
        "\n",
        "        n_features = len(existing_features)\n",
        "        print(f\" Number of features: {n_features}\")\n",
        "\n",
        "        # Check for any remaining NaN values (should be 0 after fillna)\n",
        "        nan_counts = df_features.isnull().sum()\n",
        "        if int(nan_counts.sum()) > 0:\n",
        "            print(f\" NaN counts after interpolation and fillna:\")\n",
        "            for col, count in nan_counts[nan_counts > 0].items():\n",
        "                print(f\"  {col}: {int(count)}\")\n",
        "\n",
        "        print(\"Scaling data...\")\n",
        "        scaler = StandardScaler()\n",
        "        data_scaled = scaler.fit_transform(df_features)\n",
        "        print(\"Data scaled successfully.\")\n",
        "\n",
        "        # Convert back to DataFrame with proper column names and ensure float64\n",
        "        data_scaled = pd.DataFrame(data_scaled, columns=df_features.columns, dtype=np.float64)\n",
        "\n",
        "        # Check for any infinite or NaN values in scaled data\n",
        "        inf_count = int(np.isinf(data_scaled).sum().sum())\n",
        "        nan_count = int(data_scaled.isnull().sum().sum())\n",
        "\n",
        "        if inf_count > 0:\n",
        "            print(f\" Infinite values in scaled data: {inf_count}\")\n",
        "            data_scaled = data_scaled.replace([np.inf, -np.inf], 0.0)\n",
        "\n",
        "        if nan_count > 0:\n",
        "            print(f\" NaN values in scaled data: {nan_count}\")\n",
        "\n",
        "        # Ensure all values are finite\n",
        "        data_scaled = data_scaled.fillna(0.0)\n",
        "        data_scaled = data_scaled.replace([np.inf, -np.inf], 0.0)\n",
        "\n",
        "        # Save the processed data\n",
        "        data.to_csv('processed_can_data.csv', index=False)\n",
        "        print(\" Processed data saved to 'processed_can_data.csv'\")\n",
        "\n",
        "        # Run data quality check\n",
        "        check_data_quality(data_scaled)\n",
        "\n",
        "        # Convert to numpy array with explicit float32 for TensorFlow compatibility\n",
        "        return data_scaled.values.astype(np.float32), scaler, existing_features\n",
        "\n",
        "    else:\n",
        "        print(\" No valid features found in the data!\")\n",
        "        print(\"Available columns:\", data.columns.tolist())\n",
        "        return None, None, None\n",
        "\n",
        "def create_sequences(data, timesteps=20):\n",
        "    \"\"\"Create sequences for training with proper numpy type handling\"\"\"\n",
        "    print(f\"Creating sequences with timesteps={timesteps}...\")\n",
        "\n",
        "    if data is None:\n",
        "        return None\n",
        "\n",
        "    n_samples = int(len(data) - timesteps + 1)\n",
        "    n_features = int(data.shape[1])\n",
        "\n",
        "    # Use float32 for TensorFlow compatibility\n",
        "    sequences = np.zeros((n_samples, timesteps, n_features), dtype=np.float32)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        sequences[i] = data[i:i+timesteps].astype(np.float32)\n",
        "\n",
        "    print(f\"✓ Created {n_samples} sequences with shape {sequences.shape}\")\n",
        "    return sequences\n",
        "\n",
        "def prepare_data(data, test_size=0.2, timesteps=20):\n",
        "    \"\"\"Prepare data for training with numpy type handling\"\"\"\n",
        "    global timesteps_global, n_features_global\n",
        "\n",
        "    print(\"PREPARING DATA FOR TRAINING\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create sequences\n",
        "    sequences = create_sequences(data, timesteps)\n",
        "\n",
        "    if sequences is None:\n",
        "        return None, None, None\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test = train_test_split(sequences, test_size=test_size, random_state=42)\n",
        "\n",
        "    # Ensure float32 type for TensorFlow\n",
        "    X_train = X_train.astype(np.float32)\n",
        "    X_test = X_test.astype(np.float32)\n",
        "\n",
        "    # Update global variables\n",
        "    timesteps_global = int(timesteps)\n",
        "    n_features_global = int(sequences.shape[2])\n",
        "\n",
        "    print(f\" Training data shape: {X_train.shape}\")\n",
        "    print(f\" Test data shape: {X_test.shape}\")\n",
        "\n",
        "    return X_train, X_test, int(sequences.shape[2])\n",
        "\n",
        "# ============================================================================\n",
        "# CONSTRAINED MODEL ARCHITECTURES WITH NUMPY TYPE HANDLING\n",
        "# ============================================================================\n",
        "\n",
        "def build_lstm_autoencoder_architecture(timesteps, n_features, latent_dim=64,\n",
        "                                     activation='relu', dropout_rate=0.2,\n",
        "                                     learning_rate=0.001, optimizer='adam'):\n",
        "    \"\"\"Build constrained LSTM autoencoder with proper type handling\"\"\"\n",
        "    # CONSTRAINT: Limit latent_dim to 16-64 range\n",
        "    latent_dim = max(8, min(64, int(latent_dim)))\n",
        "    timesteps = int(timesteps)\n",
        "    n_features = int(n_features)\n",
        "    dropout_rate = float(dropout_rate)\n",
        "    learning_rate = float(learning_rate)\n",
        "\n",
        "    print(f\"\\n BUILDING CONSTRAINED LSTM AUTOENCODER\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\" Input shape: ({timesteps}, {n_features})\")\n",
        "    print(f\" Latent dimension: {latent_dim}\")\n",
        "    print(f\" Activation: {activation}\")\n",
        "    print(f\" Dropout rate: {dropout_rate}\")\n",
        "    print(f\" Optimizer: {optimizer}\")\n",
        "\n",
        "\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=(timesteps, n_features), name=\"LSTM_Input\", dtype=tf.float32)\n",
        "\n",
        "    # ENCODER - Max 2 layers\n",
        "    intermediate_dim = min(64, max(latent_dim * 2, 32))\n",
        "\n",
        "    encoder = LSTM(intermediate_dim, activation=activation, return_sequences=True,\n",
        "                   name=\"Encoder_LSTM_1\", dtype=tf.float32)(input_layer)\n",
        "\n",
        "    if dropout_rate > 0:\n",
        "        encoder = Dropout(dropout_rate, name=\"Encoder_Dropout_1\")(encoder)\n",
        "\n",
        "    encoder = LSTM(latent_dim, activation=activation, return_sequences=False,\n",
        "                   name=\"Encoder_LSTM_2\", dtype=tf.float32)(encoder)\n",
        "\n",
        "    if dropout_rate > 0:\n",
        "        encoder = Dropout(dropout_rate, name=\"Encoder_Dropout_2\")(encoder)\n",
        "\n",
        "    # DECODER\n",
        "    decoder = RepeatVector(timesteps, name=\"Repeat_Vector\")(encoder)\n",
        "\n",
        "    decoder = LSTM(latent_dim, activation=activation, return_sequences=True,\n",
        "                   name=\"Decoder_LSTM_1\", dtype=tf.float32)(decoder)\n",
        "\n",
        "    if dropout_rate > 0:\n",
        "        decoder = Dropout(dropout_rate, name=\"Decoder_Dropout_1\")(decoder)\n",
        "\n",
        "    decoder = LSTM(intermediate_dim, activation=activation, return_sequences=True,\n",
        "                   name=\"Decoder_LSTM_2\", dtype=tf.float32)(decoder)\n",
        "\n",
        "    if dropout_rate > 0:\n",
        "        decoder = Dropout(dropout_rate, name=\"Decoder_Dropout_2\")(decoder)\n",
        "\n",
        "    # Output layer\n",
        "    output_layer = TimeDistributed(Dense(n_features, activation='sigmoid', dtype=tf.float32),\n",
        "                                  name=\"LSTM_Output\")(decoder)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer, name=\"Constrained_LSTM_Autoencoder\")\n",
        "\n",
        "    # Compile model\n",
        "    selected_optimizer = Adam(learning_rate=learning_rate) if optimizer == 'adam' else RMSprop(learning_rate=learning_rate)\n",
        "    model.compile(\n",
        "        optimizer=selected_optimizer,\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    total_params = int(model.count_params())\n",
        "    print(f\" Total parameters: {total_params:,}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_constrained_conv1d_autoencoder(timesteps, n_features, filters=32,\n",
        "                                        kernel_size=3, learning_rate=0.001, dropout_rate=0.2, optimizer='adam'):\n",
        "    \"\"\"Constrained Conv1D Autoencoder with proper type handling\"\"\"\n",
        "    # Convert and constrain parameters\n",
        "    filters = max(8, min(64, int(filters)))\n",
        "    kernel_size = int(kernel_size)\n",
        "    timesteps = int(timesteps)\n",
        "    n_features = int(n_features)\n",
        "    learning_rate = float(learning_rate)\n",
        "    dropout_rate = float(dropout_rate)\n",
        "\n",
        "    print(f\"\\n BUILDING CONSTRAINED CONV1D AUTOENCODER\")\n",
        "    print(f\" Filters: {filters}, Kernel: {kernel_size}, Optimizer: {optimizer}\")\n",
        "\n",
        "    input_layer = Input(shape=(timesteps, n_features), name=\"Conv1D_Input\", dtype=tf.float32)\n",
        "\n",
        "    # ENCODER\n",
        "    x = Conv1D(filters, kernel_size, activation='relu', padding='same',\n",
        "               name=\"Conv1D_1\", dtype=tf.float32)(input_layer)\n",
        "    x = BatchNormalization(name=\"BN_1\")(x)\n",
        "    if dropout_rate > 0:\n",
        "        x = Dropout(dropout_rate, name=\"Dropout_1\")(x)\n",
        "\n",
        "    x = MaxPooling1D(2, padding='same', name=\"Pool_1\")(x)\n",
        "\n",
        "    reduced_filters = max(16, filters // 2)\n",
        "    encoded = Conv1D(reduced_filters, kernel_size, activation='relu', padding='same',\n",
        "                     name=\"Conv1D_2\", dtype=tf.float32)(x)\n",
        "    encoded = BatchNormalization(name=\"BN_2\")(encoded)\n",
        "\n",
        "    # DECODER\n",
        "    x = Conv1D(reduced_filters, kernel_size, activation='relu', padding='same',\n",
        "               name=\"DeConv1D_1\", dtype=tf.float32)(encoded)\n",
        "    x = BatchNormalization(name=\"BN_3\")(x)\n",
        "    if dropout_rate > 0:\n",
        "        x = Dropout(dropout_rate, name=\"Dropout_2\")(x)\n",
        "\n",
        "    x = UpSampling1D(2, name=\"Upsample_1\")(x)\n",
        "\n",
        "    # Ensure correct timesteps\n",
        "    if x.shape[1] != timesteps:\n",
        "        if x.shape[1] < timesteps:\n",
        "            pad_needed = timesteps - x.shape[1]\n",
        "            x = tf.keras.layers.Lambda(\n",
        "                lambda x: tf.pad(x, [[0,0], [0, pad_needed], [0,0]], 'symmetric')\n",
        "            )(x)\n",
        "        else:\n",
        "            x = tf.keras.layers.Lambda(lambda x: x[:, :timesteps, :])(x)\n",
        "\n",
        "    # Final reconstruction layer\n",
        "    decoded = Conv1D(n_features, kernel_size, activation='sigmoid', padding='same',\n",
        "                     name=\"Output_Conv\", dtype=tf.float32)(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=decoded, name=\"Constrained_Conv1D_Autoencoder\")\n",
        "    selected_optimizer = Adam(learning_rate=learning_rate) if optimizer == 'adam' else RMSprop(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=selected_optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "    print(f\" Conv1D model: {int(model.count_params()):,} parameters\")\n",
        "    return model\n",
        "\n",
        "def build_constrained_gru_autoencoder(timesteps, n_features, units=32,\n",
        "                                     learning_rate=0.001, dropout_rate=0.2, optimizer='adam'):\n",
        "    \"\"\"Constrained GRU Autoencoder with proper type handling\"\"\"\n",
        "    # Convert and constrain parameters\n",
        "    units = max(8, min(64, int(units)))\n",
        "    timesteps = int(timesteps)\n",
        "    n_features = int(n_features)\n",
        "    learning_rate = float(learning_rate)\n",
        "    dropout_rate = float(dropout_rate)\n",
        "\n",
        "    print(f\"\\n BUILDING CONSTRAINED GRU AUTOENCODER\")\n",
        "    print(f\" Units: {units}, Optimizer: {optimizer}\")\n",
        "\n",
        "    input_layer = Input(shape=(timesteps, n_features), name=\"GRU_Input\", dtype=tf.float32)\n",
        "\n",
        "    # ENCODER\n",
        "    x = GRU(units, return_sequences=True, dropout=dropout_rate,\n",
        "            name=\"GRU_Encoder_1\", dtype=tf.float32)(input_layer)\n",
        "    encoder_output = GRU(max(16, units//2), return_sequences=False, dropout=dropout_rate,\n",
        "                        name=\"GRU_Encoder_2\", dtype=tf.float32)(x)\n",
        "\n",
        "    # DECODER\n",
        "    decoder = RepeatVector(timesteps, name=\"Repeat_Vector\")(encoder_output)\n",
        "    x = GRU(max(16, units//2), return_sequences=True, dropout=dropout_rate,\n",
        "            name=\"GRU_Decoder_1\", dtype=tf.float32)(x)\n",
        "    decoder_output = GRU(units, return_sequences=True, dropout=dropout_rate,\n",
        "                        name=\"GRU_Decoder_2\", dtype=tf.float32)(x)\n",
        "\n",
        "    # Output layer\n",
        "    output_layer = TimeDistributed(Dense(n_features, activation='sigmoid', dtype=tf.float32),\n",
        "                                  name=\"GRU_Output\")(decoder_output)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer, name=\"Constrained_GRU_Autoencoder\")\n",
        "    selected_optimizer = Adam(learning_rate=learning_rate) if optimizer == 'adam' else RMSprop(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=selected_optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "    print(f\" GRU model: {int(model.count_params()):,} parameters\")\n",
        "    return model\n",
        "\n",
        "def build_constrained_hybrid_autoencoder(timesteps, n_features,\n",
        "                                        conv_filters=32, lstm_units=32, gru_units=16,\n",
        "                                        learning_rate=0.001, dropout_rate=0.2, optimizer='adam'):\n",
        "    \"\"\"Constrained Hybrid Autoencoder with proper type handling\"\"\"\n",
        "    # Convert and constrain parameters\n",
        "    conv_filters = max(16, min(64, int(conv_filters)))\n",
        "    lstm_units = max(16, min(64, int(lstm_units)))\n",
        "    gru_units = max(16, min(32, int(gru_units)))\n",
        "    timesteps = int(timesteps)\n",
        "    n_features = int(n_features)\n",
        "    learning_rate = float(learning_rate)\n",
        "    dropout_rate = float(dropout_rate)\n",
        "\n",
        "    print(f\"\\n BUILDING CONSTRAINED HYBRID AUTOENCODER\")\n",
        "    print(f\" Conv filters: {conv_filters}, LSTM units: {lstm_units}, GRU units: {gru_units}, Optimizer: {optimizer}\")\n",
        "\n",
        "    input_layer = Input(shape=(timesteps, n_features), name=\"Hybrid_Input\", dtype=tf.float32)\n",
        "\n",
        "    # BRANCH 1: Conv1D path\n",
        "    conv_branch = Conv1D(conv_filters//2, 3, activation='relu', padding='same',\n",
        "                        name=\"Conv_Branch\", dtype=tf.float32)(input_layer)\n",
        "    conv_branch = Dropout(dropout_rate, name=\"Conv_Dropout\")(conv_branch)\n",
        "\n",
        "    # BRANCH 2: LSTM path\n",
        "    lstm_branch = LSTM(lstm_units, return_sequences=True, dropout=dropout_rate,\n",
        "                      name=\"LSTM_Branch\", dtype=tf.float32)(input_layer)\n",
        "\n",
        "    # BRANCH 3: GRU path\n",
        "    gru_branch = GRU(gru_units, return_sequences=True, dropout=dropout_rate,\n",
        "                    name=\"GRU_Branch\", dtype=tf.float32)(input_layer)\n",
        "\n",
        "    # FUSION\n",
        "    combined = Concatenate(name=\"Fusion\")([conv_branch, lstm_branch, gru_branch])\n",
        "\n",
        "    # ENCODER\n",
        "    encoder = LSTM(lstm_units, return_sequences=False, dropout=dropout_rate,\n",
        "                  name=\"Encoder\", dtype=tf.float32)(combined)\n",
        "\n",
        "    # DECODER\n",
        "    decoder = RepeatVector(timesteps, name=\"Decoder_Repeat\")(encoder)\n",
        "    decoder = LSTM(lstm_units, return_sequences=True, dropout=dropout_rate,\n",
        "                  name=\"Decoder_LSTM\", dtype=tf.float32)(decoder)\n",
        "\n",
        "    # OUTPUT\n",
        "    output_layer = TimeDistributed(Dense(n_features, activation='sigmoid', dtype=tf.float32),\n",
        "                                  name=\"Hybrid_Output\")(decoder)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer, name=\"Constrained_Hybrid_Autoencoder\")\n",
        "    selected_optimizer = Adam(learning_rate=learning_rate) if optimizer == 'adam' else RMSprop(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=selected_optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "    print(f\" Hybrid model: {int(model.count_params()):,} parameters\")\n",
        "    return model\n",
        "\n",
        "def build_constrained_attention_autoencoder(timesteps, n_features, embed_dim=32,\n",
        "                                           num_heads=2, learning_rate=0.001, dropout_rate=0.2, optimizer='adam'):\n",
        "    \"\"\"Constrained Attention Autoencoder with proper type handling\"\"\"\n",
        "    # Convert and constrain parameters\n",
        "    embed_dim = max(8, min(64, int(embed_dim)))\n",
        "    num_heads = max(1, min(4, int(num_heads)))\n",
        "    timesteps = int(timesteps)\n",
        "    n_features = int(n_features)\n",
        "    learning_rate = float(learning_rate)\n",
        "    dropout_rate = float(dropout_rate)\n",
        "\n",
        "    print(f\"\\n BUILDING CONSTRAINED ATTENTION AUTOENCODER\")\n",
        "    print(f\" Embed dim: {embed_dim}, Attention heads: {num_heads}, Optimizer: {optimizer}\")\n",
        "\n",
        "    input_layer = Input(shape=(timesteps, n_features), name=\"Attention_Input\", dtype=tf.float32)\n",
        "\n",
        "    # PROJECT TO EMBEDDING\n",
        "    x = Dense(embed_dim, name=\"Input_Projection\", dtype=tf.float32)(input_layer)\n",
        "\n",
        "    # ATTENTION LAYER 1\n",
        "    attention_output = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim//num_heads, name=\"Attention_1\")(x, x)\n",
        "    x = Add(name=\"Add_1\")([x, attention_output])\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"Norm_1\")(x)\n",
        "\n",
        "    # ENCODER COMPRESSION\n",
        "    encoder = LSTM(embed_dim//2, return_sequences=False, dropout=dropout_rate,\n",
        "                  name=\"Encoder_Compress\", dtype=tf.float32)(x)\n",
        "\n",
        "    # DECODER EXPANSION\n",
        "    decoder = RepeatVector(timesteps, name=\"Decoder_Repeat\")(encoder)\n",
        "    decoder = LSTM(embed_dim, return_sequences=True, dropout=dropout_rate,\n",
        "                  name=\"Decoder_Expand\", dtype=tf.float32)(decoder)\n",
        "\n",
        "    # ATTENTION LAYER 2\n",
        "    attention_output_2 = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=max(1, num_heads//2), key_dim=embed_dim//max(1, num_heads//2),\n",
        "        name=\"Attention_2\")(decoder, decoder)\n",
        "    decoder = Add(name=\"Add_2\")([decoder, attention_output_2])\n",
        "    decoder = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"Norm_2\")(decoder)\n",
        "\n",
        "    # OUTPUT PROJECTION\n",
        "    output_layer = TimeDistributed(Dense(n_features, activation='sigmoid', dtype=tf.float32),\n",
        "                                  name=\"Attention_Output\")(decoder)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer, name=\"Constrained_Attention_Autoencoder\")\n",
        "    selected_optimizer = Adam(learning_rate=learning_rate) if optimizer == 'adam' else RMSprop(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=selected_optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "    print(f\" Attention model: {int(model.count_params()):,} parameters\")\n",
        "    return model\n",
        "\n",
        "# Builder functions for Keras Tuner with type conversion\n",
        "def build_lstm_autoencoder(hp):\n",
        "    \"\"\"Build LSTM autoencoder with hyperparameters and type conversion\"\"\"\n",
        "    return build_lstm_autoencoder_architecture(\n",
        "        timesteps=timesteps_global,\n",
        "        n_features=n_features_global,\n",
        "        latent_dim=int(hp.Int('latent_dim', min_value=8, max_value=64, step=8)),\n",
        "        activation=hp.Choice('activation', values=['relu', 'tanh']),\n",
        "        dropout_rate=float(hp.Float('dropout_rate', min_value=0.0, max_value=0.4, step=0.1)),\n",
        "        learning_rate=float(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "        optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop'])\n",
        "    )\n",
        "\n",
        "def build_conv1d_autoencoder(hp):\n",
        "    \"\"\"Build Conv1D autoencoder with hyperparameters and type conversion\"\"\"\n",
        "    return build_constrained_conv1d_autoencoder(\n",
        "        timesteps=timesteps_global,\n",
        "        n_features=n_features_global,\n",
        "        filters=int(hp.Int('filters', min_value=8, max_value=64, step=8)),\n",
        "        kernel_size=int(hp.Choice('kernel_size', values=[3, 5])),\n",
        "        dropout_rate=float(hp.Float('dropout_rate', min_value=0.0, max_value=0.4, step=0.1)),\n",
        "        learning_rate=float(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "        optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop'])\n",
        "    )\n",
        "\n",
        "def build_gru_autoencoder(hp):\n",
        "    \"\"\"Build GRU autoencoder with hyperparameters and type conversion\"\"\"\n",
        "    return build_constrained_gru_autoencoder(\n",
        "        timesteps=timesteps_global,\n",
        "        n_features=n_features_global,\n",
        "        units=int(hp.Int('units', min_value=8, max_value=64, step=8)),\n",
        "        dropout_rate=float(hp.Float('dropout_rate', min_value=0.0, max_value=0.4, step=0.1)),\n",
        "        learning_rate=float(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "        optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop'])\n",
        "    )\n",
        "\n",
        "def build_hybrid_autoencoder(hp):\n",
        "    \"\"\"Build Hybrid autoencoder with hyperparameters and type conversion\"\"\"\n",
        "    return build_constrained_hybrid_autoencoder(\n",
        "        timesteps=timesteps_global,\n",
        "        n_features=n_features_global,\n",
        "        conv_filters=int(hp.Int('conv_filters', min_value=8, max_value=64, step=8)),\n",
        "        lstm_units=int(hp.Int('lstm_units', min_value=8, max_value=64, step=8)),\n",
        "        gru_units=int(hp.Int('gru_units', min_value=8, max_value=32, step=8)),\n",
        "        dropout_rate=float(hp.Float('dropout_rate', min_value=0.0, max_value=0.4, step=0.1)),\n",
        "        learning_rate=float(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "        optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop'])\n",
        "    )\n",
        "\n",
        "def build_attention_autoencoder(hp):\n",
        "    \"\"\"Build Attention autoencoder with hyperparameters and type conversion\"\"\"\n",
        "    return build_constrained_attention_autoencoder(\n",
        "        timesteps=timesteps_global,\n",
        "        n_features=n_features_global,\n",
        "        embed_dim=int(hp.Int('embed_dim', min_value=8, max_value=64, step=8)),\n",
        "        num_heads=int(hp.Int('num_heads', min_value=1, max_value=4)),\n",
        "        dropout_rate=float(hp.Float('dropout_rate', min_value=0.0, max_value=0.4, step=0.1)),\n",
        "        learning_rate=float(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "        optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop'])\n",
        "    )\n",
        "\n",
        "\n",
        "def create_multi_model_tuners():\n",
        "    \"\"\"Create tuners for all five models\"\"\"\n",
        "    models_config = {\n",
        "        'LSTM': {\n",
        "            'builder': build_lstm_autoencoder,\n",
        "            'directory': 'lstm_tuning',\n",
        "            'project': 'can_lstm_optimization'\n",
        "        },\n",
        "        'Conv1D': {\n",
        "            'builder': build_conv1d_autoencoder,\n",
        "            'directory': 'conv1d_tuning',\n",
        "            'project': 'can_conv1d_optimization'\n",
        "        },\n",
        "        'GRU': {\n",
        "            'builder': build_gru_autoencoder,\n",
        "            'directory': 'gru_tuning',\n",
        "            'project': 'can_gru_optimization'\n",
        "        },\n",
        "        'Hybrid': {\n",
        "            'builder': build_hybrid_autoencoder,\n",
        "            'directory': 'hybrid_tuning',\n",
        "            'project': 'can_hybrid_optimization'\n",
        "        },\n",
        "        'Attention': {\n",
        "            'builder': build_attention_autoencoder,\n",
        "            'directory': 'attention_tuning',\n",
        "            'project': 'can_attention_optimization'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    tuners = {}\n",
        "\n",
        "    for model_name, config in models_config.items():\n",
        "        # Clean directory if exists\n",
        "        if os.path.exists(config['directory']):\n",
        "            import shutil\n",
        "            shutil.rmtree(config['directory'])\n",
        "\n",
        "        tuners[model_name] = kt.RandomSearch(\n",
        "            config['builder'],\n",
        "            objective='val_loss',\n",
        "            max_trials=25, # Increased number of trials\n",
        "            directory=config['directory'],\n",
        "            project_name=config['project'],\n",
        "            overwrite=True\n",
        "        )\n",
        "\n",
        "    return tuners\n",
        "\n",
        "def safe_shape_align(reconstructions, target_shape):\n",
        "    \"\"\"Safely align reconstruction shapes with target\"\"\"\n",
        "    if reconstructions.shape == target_shape:\n",
        "        return reconstructions.astype(np.float32), True\n",
        "\n",
        "    print(f\"⚠ Shape mismatch: {reconstructions.shape} vs {target_shape}\")\n",
        "\n",
        "    # Convert to float32 first\n",
        "    reconstructions = reconstructions.astype(np.float32)\n",
        "\n",
        "    # Try to fix common issues\n",
        "    if len(reconstructions.shape) == len(target_shape):\n",
        "        if reconstructions.shape[-1] == target_shape[-1]:  # Same features\n",
        "            # Fix timesteps dimension\n",
        "            if reconstructions.shape[1] > target_shape[1]:\n",
        "                reconstructions = reconstructions[:, :target_shape[1], :]\n",
        "            elif reconstructions.shape[1] < target_shape[1]:\n",
        "                pad_needed = target_shape[1] - reconstructions.shape[1]\n",
        "                reconstructions = np.pad(reconstructions, ((0,0), (0, pad_needed), (0,0)), 'constant')\n",
        "\n",
        "            if reconstructions.shape == target_shape:\n",
        "                return reconstructions.astype(np.float32), True\n",
        "\n",
        "    print(f\" Could not align shapes\")\n",
        "    return reconstructions.astype(np.float32), False\n",
        "\n",
        "def train_and_evaluate_multi_models(X_train, X_test, timesteps_val, n_features_val):\n",
        "    \"\"\"Train and evaluate all five models with detailed comparison and type safety\"\"\"\n",
        "    global timesteps_global, n_features_global, X_test_global\n",
        "    timesteps_global = int(timesteps_val)\n",
        "    n_features_global = int(n_features_val)\n",
        "    X_test_global = X_test.astype(np.float32)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" MULTI-MODEL CAN DATA AUTOENCODER COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Models: LSTM, Conv1D, GRU, Hybrid (Conv1D+LSTM+GRU), Attention\")\n",
        "    print(f\"Data shape: {X_train.shape} -> {X_test.shape}\")\n",
        "    print(f\"Features: {n_features_val}, Timesteps: {timesteps_val}\")\n",
        "\n",
        "    # Ensure data types\n",
        "    X_train = X_train.astype(np.float32)\n",
        "    X_test = X_test.astype(np.float32)\n",
        "\n",
        "    # Create tuners\n",
        "    tuners = create_multi_model_tuners()\n",
        "\n",
        "    # Results storage\n",
        "    results = {}\n",
        "\n",
        "    # Search callbacks\n",
        "    search_callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=0),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-7, verbose=0)\n",
        "    ]\n",
        "\n",
        "    # Train each model\n",
        "    for model_name, tuner in tuners.items():\n",
        "        print(f\"\\n\" + \"=\"*70)\n",
        "        print(f\" TRAINING {model_name.upper()} AUTOENCODER\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        try:\n",
        "            # Hyperparameter search\n",
        "            print(f\" Starting hyperparameter search for {model_name}...\")\n",
        "            tuner.search(\n",
        "                X_train, X_train,\n",
        "                epochs=30,\n",
        "                validation_data=(X_test, X_test),\n",
        "                callbacks=search_callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Check if any trials were completed successfully\n",
        "            if not tuner.oracle.get_best_trials(1):\n",
        "                 print(f\" Hyperparameter search failed for {model_name}. No successful trials.\")\n",
        "                 raise ValueError(\"No successful trials in hyperparameter search\")\n",
        "\n",
        "            # Get best model and hyperparameters\n",
        "            best_trial = tuner.oracle.get_best_trials(1)[0]\n",
        "            best_hps = best_trial.hyperparameters\n",
        "            best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "            best_val_loss = float(best_trial.metrics.get_last_value('val_loss'))\n",
        "\n",
        "            print(f\" Best hyperparameters found for {model_name}\")\n",
        "            print(f\"   Validation loss: {best_val_loss:.6f}\")\n",
        "            # Safely access batch_size as it might not always be tuned\n",
        "            best_batch_size = best_hps.get('batch_size', 32)\n",
        "            print(f\"   Batch Size: {best_batch_size}\")\n",
        "            print(f\"   Optimizer: {best_hps.get('optimizer', 'N/A')}\")\n",
        "\n",
        "\n",
        "            # Extended training\n",
        "            print(f\" Extended training for {model_name}...\")\n",
        "\n",
        "            extended_callbacks = [\n",
        "                EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, min_lr=1e-8, verbose=1)\n",
        "            ]\n",
        "\n",
        "            history = best_model.fit(\n",
        "                X_train, X_train,\n",
        "                epochs=100, # Increased maximum epochs for extended training\n",
        "                batch_size=best_batch_size,\n",
        "                validation_data=(X_test, X_test),\n",
        "                callbacks=extended_callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Evaluate model\n",
        "            print(f\" Evaluating {model_name}...\")\n",
        "            test_loss, test_mae = best_model.evaluate(X_test, X_test, verbose=0)\n",
        "\n",
        "            # Convert to native Python types\n",
        "            test_loss = float(test_loss)\n",
        "            test_mae = float(test_mae)\n",
        "\n",
        "            reconstructions = best_model.predict(X_test, verbose=0)\n",
        "\n",
        "            # Safely align shapes\n",
        "            reconstructions, shape_ok = safe_shape_align(reconstructions, X_test.shape)\n",
        "\n",
        "            # Calculate additional metrics if shapes match\n",
        "            if shape_ok:\n",
        "                # Convert to numpy arrays and ensure float32\n",
        "                X_test_np = X_test.astype(np.float32)\n",
        "                reconstructions_np = reconstructions.astype(np.float32)\n",
        "\n",
        "                mse = float(np.mean(np.square(X_test_np - reconstructions_np)))\n",
        "                rmse = float(np.sqrt(mse))\n",
        "\n",
        "                # Calculate R² score\n",
        "                ss_res = float(np.sum(np.square(X_test_np - reconstructions_np)))\n",
        "                ss_tot = float(np.sum(np.square(X_test_np - np.mean(X_test_np))))\n",
        "                r2_score = float(1 - (ss_res / (ss_tot + 1e-8)))\n",
        "\n",
        "                # Calculate mean absolute percentage error\n",
        "                mape = float(np.mean(np.abs((X_test_np - reconstructions_np) / (X_test_np + 1e-8))) * 100)\n",
        "            else:\n",
        "                print(f\" Could not calculate advanced metrics for {model_name} due to shape mismatch\")\n",
        "                mse, rmse, r2_score, mape = np.nan, np.nan, np.nan, np.nan\n",
        "\n",
        "            # Store results with type conversion\n",
        "            results[model_name] = {\n",
        "                'model': best_model,\n",
        "                'history': history,\n",
        "                'hyperparameters': best_hps.values, # Store hyperparameters as dictionary\n",
        "                'test_loss': safe_float_conversion(test_loss),\n",
        "                'test_mae': safe_float_conversion(test_mae),\n",
        "                'mse': safe_float_conversion(mse),\n",
        "                'rmse': safe_float_conversion(rmse),\n",
        "                'r2_score': safe_float_conversion(r2_score),\n",
        "                'mape': safe_float_conversion(mape),\n",
        "                'reconstructions': reconstructions,\n",
        "                'tuner': tuner,\n",
        "                'training_epochs': int(len(history.history['loss'])),\n",
        "                'parameters': int(best_model.count_params()),\n",
        "                'shape_aligned': bool(shape_ok)\n",
        "            }\n",
        "\n",
        "            print(f\" {model_name} completed successfully!\")\n",
        "            print(f\"   Test Loss: {test_loss:.6f}\")\n",
        "            if not np.isnan(r2_score):\n",
        "                print(f\"   R² Score: {r2_score:.4f}\")\n",
        "            print(f\"   Parameters: {int(best_model.count_params()):,}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error training {model_name}: {str(e)}\")\n",
        "            print(f\"   Skipping {model_name}...\")\n",
        "            continue\n",
        "\n",
        "    if not results:\n",
        "        print(\" No models were successfully trained!\")\n",
        "        return None\n",
        "\n",
        "    return results\n",
        "\n",
        "def comprehensive_model_comparison(results):\n",
        "    \"\"\"Comprehensive comparison of all trained models with type safety\"\"\"\n",
        "    if not results:\n",
        "        print(\" No results to compare!\")\n",
        "        return None, None\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" COMPREHENSIVE MODEL PERFORMANCE COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create detailed comparison DataFrame with type conversion\n",
        "    comparison_data = []\n",
        "    for model_name, result in results.items():\n",
        "        comparison_data.append({\n",
        "            'Model': str(model_name),\n",
        "            'Test_Loss_MSE': safe_float_conversion(result.get('test_loss', np.nan)),\n",
        "            'Test_MAE': safe_float_conversion(result.get('test_mae', np.nan)),\n",
        "            'RMSE': safe_float_conversion(result.get('rmse', np.nan)),\n",
        "            'R²_Score': safe_float_conversion(result.get('r2_score', np.nan)),\n",
        "            'MAPE_%': safe_float_conversion(result.get('mape', np.nan)),\n",
        "            'Training_Epochs': int(result.get('training_epochs', 0)),\n",
        "            'Parameters': int(result.get('parameters', 0)),\n",
        "            'Params_K': safe_float_conversion(result.get('parameters', 0) / 1000) if result.get('parameters') else np.nan,\n",
        "            'Shape_Aligned': bool(result.get('shape_aligned', False))\n",
        "        })\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    comparison_df = comparison_df.sort_values('Test_Loss_MSE', na_position='last')\n",
        "\n",
        "    print(\"\\n PERFORMANCE RANKING (sorted by Test Loss):\")\n",
        "    print(\"-\" * 80)\n",
        "    print(comparison_df.to_string(index=False, float_format='%.6f'))\n",
        "\n",
        "    # Determine best model for different criteria\n",
        "    best_overall = None\n",
        "    best_r2 = None\n",
        "    most_efficient = None\n",
        "\n",
        "    if not comparison_df['Test_Loss_MSE'].isnull().all():\n",
        "        best_overall = str(comparison_df.iloc[0]['Model'])\n",
        "        print(f\"\\n BEST MODELS BY CRITERIA:\")\n",
        "        print(\"-\"* 40)\n",
        "        print(f\" Overall (Test Loss): {best_overall}\")\n",
        "        print(f\"   Loss: {comparison_df.iloc[0]['Test_Loss_MSE']:.6f}\")\n",
        "        if not np.isnan(comparison_df.iloc[0]['R²_Score']):\n",
        "            print(f\"   R²: {comparison_df.iloc[0]['R²_Score']:.4f}\")\n",
        "\n",
        "    if not comparison_df['R²_Score'].isnull().all():\n",
        "        best_r2_idx = comparison_df['R²_Score'].idxmax()\n",
        "        best_r2 = str(comparison_df.loc[best_r2_idx, 'Model'])\n",
        "        if best_r2 != best_overall:\n",
        "             print(f\"\\n Best R² Score: {best_r2}\")\n",
        "             r2_row = comparison_df.loc[best_r2_idx]\n",
        "             print(f\"   R²: {r2_row['R²_Score']:.4f}\")\n",
        "             print(f\"   Loss: {r2_row['Test_Loss_MSE']:.6f}\")\n",
        "\n",
        "    if not comparison_df['Parameters'].isnull().all():\n",
        "        most_efficient_idx = comparison_df['Parameters'].idxmin()\n",
        "        most_efficient = str(comparison_df.loc[most_efficient_idx, 'Model'])\n",
        "        if most_efficient != best_overall and most_efficient != best_r2:\n",
        "            print(f\"\\n Most Efficient (Parameters): {most_efficient}\")\n",
        "            eff_row = comparison_df.loc[most_efficient_idx]\n",
        "            print(f\"   Parameters: {int(eff_row['Parameters']):,}\")\n",
        "            print(f\"   Loss: {eff_row['Test_Loss_MSE']:.6f}\")\n",
        "\n",
        "    return comparison_df, best_overall\n",
        "\n",
        "def save_multi_model_results(results, comparison_df, best_model_name):\n",
        "    \"\"\"Save all comparison results and models with type safety\"\"\"\n",
        "    if not results:\n",
        "        print(\" No results to save!\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" SAVING MULTI-MODEL RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Save comparison DataFrame\n",
        "    if comparison_df is not None:\n",
        "        comparison_df.to_csv('multi_model_performance_comparison.csv', index=False)\n",
        "        print(\"Performance comparison saved to 'multi_model_performance_comparison.csv'\")\n",
        "\n",
        "    # Save best model\n",
        "    if best_model_name and best_model_name in results:\n",
        "        try:\n",
        "            best_model = results[best_model_name]['model']\n",
        "            best_model.save(f'best_model_{best_model_name.lower()}_autoencoder.h5')\n",
        "            print(f\" Best model ({best_model_name}) saved\")\n",
        "        except Exception as e:\n",
        "            print(f\" Error saving best model {best_model_name}: {e}\")\n",
        "\n",
        "    # Save all models and their data\n",
        "    for model_name, result in results.items():\n",
        "        try:\n",
        "            # Save model\n",
        "            if 'model' in result:\n",
        "                result['model'].save(f'can_{model_name.lower()}_autoencoder.h5')\n",
        "                print(f\" {model_name} model saved\")\n",
        "\n",
        "            # Save hyperparameters with type conversion\n",
        "            if 'hyperparameters' in result:\n",
        "                # Hyperparameters are already a dictionary from best_hps.values\n",
        "                hp_dict = convert_numpy_types(result['hyperparameters'])\n",
        "\n",
        "                # Save as JSON for better type handling\n",
        "                with open(f'can_{model_name.lower()}_hyperparameters.json', 'w') as f:\n",
        "                    json.dump(hp_dict, f, indent=2)\n",
        "                print(f\" {model_name} hyperparameters saved\")\n",
        "\n",
        "            # Save training history with type conversion\n",
        "            if 'history' in result and result['history']:\n",
        "                history_dict = {}\n",
        "                for key, values in result['history'].history.items():\n",
        "                    history_dict[key] = [convert_numpy_types(v) for v in values]\n",
        "\n",
        "                history_df = pd.DataFrame(history_dict)\n",
        "                history_df.to_csv(f'can_{model_name.lower()}_training_history.csv', index=False)\n",
        "                print(f\" {model_name} training history saved\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error saving results for {model_name}: {e}\")\n",
        "\n",
        "    # Save comprehensive report with type conversion\n",
        "    try:\n",
        "        with open('can_multi_model_comparison_report.txt', 'w') as f:\n",
        "            f.write(\"CAN DATA MULTI-MODEL AUTOENCODER COMPARISON REPORT\\n\")\n",
        "            f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "\n",
        "            f.write(\"MODELS COMPARED:\\n\")\n",
        "            f.write(\"- LSTM Autoencoder: Traditional recurrent approach\\n\")\n",
        "            f.write(\"- Conv1D Autoencoder: Spatial pattern detection\\n\")\n",
        "            f.write(\"- GRU Autoencoder: Efficient recurrent architecture\\n\")\n",
        "            f.write(\"- Hybrid Autoencoder (Conv1D+LSTM+GRU): Combined approach\\n\")\n",
        "            f.write(\"- Attention Autoencoder: Self-attention mechanism\\n\\n\")\n",
        "\n",
        "            f.write(\"PERFORMANCE RANKING:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            if comparison_df is not None:\n",
        "                for i, row in comparison_df.iterrows():\n",
        "                    f.write(f\"{i+1}. {row['Model']}\\n\")\n",
        "                    test_loss = safe_float_conversion(row['Test_Loss_MSE'])\n",
        "                    r2_score = safe_float_conversion(row['R²_Score'])\n",
        "                    params = int(row['Parameters']) if not np.isnan(row['Parameters']) else 0\n",
        "\n",
        "                    f.write(f\"   Test Loss: {test_loss:.6f}\\n\")\n",
        "                    if not np.isnan(r2_score):\n",
        "                        f.write(f\"   R² Score: {r2_score:.4f}\\n\")\n",
        "                    f.write(f\"   Parameters: {params:,}\\n\\n\")\n",
        "\n",
        "            f.write(f\"RECOMMENDED MODEL: {best_model_name if best_model_name else 'N/A'}\\n\")\n",
        "\n",
        "        print(\" Comprehensive report saved to 'can_multi_model_comparison_report.txt'\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error saving report: {e}\")\n",
        "\n",
        "def create_comprehensive_visualizations(results, feature_names):\n",
        "    \"\"\"Create comprehensive comparison visualizations with type safety\"\"\"\n",
        "    if not results:\n",
        "        print(\" No results to visualize!\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" CREATING COMPREHENSIVE VISUALIZATIONS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        model_names = list(results.keys())\n",
        "        colors = ['blue', 'red', 'green', 'orange', 'purple'][:len(model_names)]\n",
        "\n",
        "        # Create large figure with multiple subplots\n",
        "        fig = plt.figure(figsize=(25, 20))\n",
        "\n",
        "        # 1. Training Loss Comparison\n",
        "        plt.subplot(4, 4, 1)\n",
        "        for i, (model_name, result) in enumerate(results.items()):\n",
        "            if 'history' in result and result['history']:\n",
        "                loss_values = [safe_float_conversion(v) for v in result['history'].history['loss']]\n",
        "                plt.plot(loss_values, label=f'{model_name} Train', color=colors[i], linewidth=2)\n",
        "\n",
        "                if 'val_loss' in result['history'].history:\n",
        "                    val_loss_values = [safe_float_conversion(v) for v in result['history'].history['val_loss']]\n",
        "                    plt.plot(val_loss_values, label=f'{model_name} Val',\n",
        "                            color=colors[i], linewidth=2, linestyle='--')\n",
        "\n",
        "        plt.title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss (MSE)')\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. Performance Metrics Bar Chart\n",
        "        plt.subplot(4, 4, 2)\n",
        "        test_losses = [safe_float_conversion(results[name].get('test_loss', np.nan)) for name in model_names]\n",
        "        rmses = [safe_float_conversion(results[name].get('rmse', np.nan)) for name in model_names]\n",
        "        r2_scores = [safe_float_conversion(results[name].get('r2_score', np.nan)) for name in model_names]\n",
        "\n",
        "        # Normalize for visualization (excluding NaNs)\n",
        "        valid_test_losses = np.array([loss for loss in test_losses if not np.isnan(loss)])\n",
        "        valid_rmses = np.array([rmse for rmse in rmses if not np.isnan(rmse)])\n",
        "\n",
        "        if valid_test_losses.size > 0:\n",
        "            test_losses_norm = np.array(test_losses) / max(valid_test_losses)\n",
        "        else:\n",
        "            test_losses_norm = np.array(test_losses)\n",
        "\n",
        "        if valid_rmses.size > 0:\n",
        "            rmses_norm = np.array(rmses) / max(valid_rmses)\n",
        "        else:\n",
        "            rmses_norm = np.array(rmses)\n",
        "\n",
        "        x = np.arange(len(model_names))\n",
        "        width = 0.25\n",
        "\n",
        "        plt.bar(x - width, test_losses_norm, width, label='Test Loss (norm)', alpha=0.7)\n",
        "        plt.bar(x, rmses_norm, width, label='RMSE (norm)', alpha=0.7)\n",
        "        plt.bar(x + width, r2_scores, width, label='R² Score', alpha=0.7)\n",
        "\n",
        "        plt.title('Performance Metrics', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Models')\n",
        "        plt.ylabel('Normalized Values')\n",
        "        plt.xticks(x, model_names, rotation=45)\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\" Visualizations created successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error creating visualizations: {e}\")\n",
        "\n",
        "def multi_model_can_pipeline():\n",
        "    \"\"\"Complete pipeline for comparing multiple autoencoder architectures on CAN data\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" CAN DATA MULTI-MODEL AUTOENCODER COMPARISON PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "    print(\" Analyzing CAN bus data with 5 different autoencoder architectures:\")\n",
        "    print(\"   1. LSTM Autoencoder\")\n",
        "    print(\"   2. Conv1D Autoencoder\")\n",
        "    print(\"   3. GRU Autoencoder\")\n",
        "    print(\"   4. Hybrid Autoencoder (Conv1D+LSTM+GRU)\")\n",
        "    print(\"   5. Attention Autoencoder\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. Prepare CAN data\n",
        "    data_scaled, scaler, feature_names = prepare_can_data()\n",
        "\n",
        "    if data_scaled is None:\n",
        "        print(\" Failed to prepare data. Exiting.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # 2. Prepare sequences\n",
        "    timesteps = 20\n",
        "    X_train, X_test, n_features = prepare_data(data_scaled, timesteps=timesteps)\n",
        "\n",
        "    if X_train is None or X_test is None or n_features is None:\n",
        "        print(\" Failed to prepare training data. Exiting.\")\n",
        "        return None, None, None\n",
        "\n",
        "    print(f\" Data prepared successfully:\")\n",
        "    print(f\"   Training sequences: {X_train.shape}\")\n",
        "    print(f\"   Test sequences: {X_test.shape}\")\n",
        "    print(f\"   Features: {n_features}\")\n",
        "    print(f\"   Timesteps: {timesteps}\")\n",
        "\n",
        "    # 3. Train and evaluate all models\n",
        "    results = train_and_evaluate_multi_models(X_train, X_test, timesteps, n_features)\n",
        "\n",
        "    if results is None or not results:\n",
        "        print(\" No models were successfully trained!\")\n",
        "        return None, None, None\n",
        "\n",
        "    # 4. Compare performance\n",
        "    comparison_df, best_model_name = comprehensive_model_comparison(results)\n",
        "\n",
        "    # 5. Create visualizations\n",
        "    if results:\n",
        "        create_comprehensive_visualizations(results, feature_names)\n",
        "\n",
        "    # 6. Save everything\n",
        "    if results:\n",
        "        save_multi_model_results(results, comparison_df, best_model_name)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" MULTI-MODEL CAN DATA ANALYSIS COMPLETED!\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\" Best Overall Model: {best_model_name if best_model_name else 'N/A'}\")\n",
        "    print(f\" Total Models Trained: {len(results)}\")\n",
        "    print(f\" Results saved to multiple files\")\n",
        "\n",
        "    print(\"\\n Generated Files:\")\n",
        "    print(\"- multi_model_performance_comparison.csv\")\n",
        "    print(\"- can_multi_model_comparison_report.txt\")\n",
        "    print(\"- best_model_[name]_autoencoder.h5\")\n",
        "    print(\"- can_[model]_autoencoder.h5 (for each model)\")\n",
        "    print(\"- can_[model]_hyperparameters.json\")\n",
        "    print(\"- can_[model]_training_history.csv\")\n",
        "\n",
        "    return results, comparison_df, best_model_name\n",
        "\n",
        "# Run the complete pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Test constrained architectures first\n",
        "    print(\" TESTING CONSTRAINED ARCHITECTURES\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create dummy data for testing\n",
        "    test_timesteps = 20\n",
        "    test_n_features = 10\n",
        "\n",
        "    test_models = {}\n",
        "\n",
        "    try:\n",
        "        test_models['LSTM'] = build_lstm_autoencoder_architecture(test_timesteps, test_n_features, latent_dim=32)\n",
        "        test_models['Conv1D'] = build_constrained_conv1d_autoencoder(test_timesteps, test_n_features, filters=32)\n",
        "        test_models['GRU'] = build_constrained_gru_autoencoder(test_timesteps, test_n_features, units=32)\n",
        "        test_models['Hybrid'] = build_constrained_hybrid_autoencoder(test_timesteps, test_n_features)\n",
        "        test_models['Attention'] = build_constrained_attention_autoencoder(test_timesteps, test_n_features, embed_dim=32)\n",
        "\n",
        "        print(\"\\n ALL CONSTRAINED MODELS BUILT SUCCESSFULLY\")\n",
        "        print(\"=\" * 60)\n",
        "        for name, model in test_models.items():\n",
        "            print(f\"{name:10}: {int(model.count_params()):,} parameters\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error building constrained models: {e}\")\n",
        "        test_models = {}\n",
        "\n",
        "    # Run the enhanced multi-model pipeline\n",
        "    results, comparison, best_model = multi_model_can_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JE3nEVAoCwCZ",
        "outputId": "3863fcc2-ed44-4b46-ede6-47240563545a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 25 Complete [00h 00m 43s]\n",
            "val_loss: 0.45371848344802856\n",
            "\n",
            "Best val_loss So Far: 0.4520763158798218\n",
            "Total elapsed time: 00h 16m 37s\n",
            "\n",
            " BUILDING CONSTRAINED CONV1D AUTOENCODER\n",
            " Filters: 64, Kernel: 5, Optimizer: rmsprop\n",
            " Conv1D model: 22,253 parameters\n",
            " Best hyperparameters found for Conv1D\n",
            "   Validation loss: 0.452076\n",
            " Error training Conv1D: HyperParameters.get() takes 2 positional arguments but 3 were given\n",
            "   Skipping Conv1D...\n",
            "\n",
            "======================================================================\n",
            " TRAINING GRU AUTOENCODER\n",
            "======================================================================\n",
            " Starting hyperparameter search for GRU...\n",
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "8                 |8                 |units\n",
            "0.3               |0.3               |dropout_rate\n",
            "0.0014431         |0.0014431         |learning_rate\n",
            "adam              |adam              |optimizer\n",
            "\n",
            "\n",
            " BUILDING CONSTRAINED GRU AUTOENCODER\n",
            " Units: 8, Optimizer: adam\n",
            " GRU model: 2,541 parameters\n",
            "Epoch 1/30\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - loss: 1.1458 - mae: 0.9370 - val_loss: 0.9680 - val_mae: 0.8297 - learning_rate: 0.0014\n",
            "Epoch 2/30\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.9770 - mae: 0.8205 - val_loss: 0.9425 - val_mae: 0.8025 - learning_rate: 0.0014\n",
            "Epoch 3/30\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.9587 - mae: 0.7994 - val_loss: 0.9364 - val_mae: 0.7933 - learning_rate: 0.0014\n",
            "Epoch 4/30\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 0.9536 - mae: 0.7919 - val_loss: 0.9284 - val_mae: 0.7900 - learning_rate: 0.0014\n",
            "Epoch 5/30\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.9409 - mae: 0.7919 - val_loss: 0.9003 - val_mae: 0.7829 - learning_rate: 0.0014\n",
            "Epoch 6/30\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.9155 - mae: 0.7879 - val_loss: 0.8320 - val_mae: 0.7708 - learning_rate: 0.0014\n",
            "Epoch 7/30\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.8550 - mae: 0.7712 - val_loss: 0.7737 - val_mae: 0.7433 - learning_rate: 0.0014\n",
            "Epoch 8/30\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.7946 - mae: 0.7434 - val_loss: 0.7401 - val_mae: 0.7231 - learning_rate: 0.0014\n",
            "Epoch 9/30\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.7646 - mae: 0.7260 - val_loss: 0.7139 - val_mae: 0.7124 - learning_rate: 0.0014\n",
            "Epoch 10/30\n",
            "\u001b[1m111/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.7359 - mae: 0.7150"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-3057412656.py\", line 1237, in <cell line: 0>\n",
            "    results, comparison, best_model = multi_model_can_pipeline()\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3057412656.py\", line 1174, in multi_model_can_pipeline\n",
            "    results = train_and_evaluate_multi_models(X_train, X_test, timesteps, n_features)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3057412656.py\", line 794, in train_and_evaluate_multi_models\n",
            "    tuner.search(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 234, in search\n",
            "    self._try_run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n",
            "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n",
            "    return model.fit(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n",
            "    logs = self.train_function(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n",
            "    opt_outputs = multi_step_on_iterator(iterator)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 878, in _call\n",
            "    results = tracing_compilation.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 139, in call_function\n",
            "    return function._call_flat(  # pylint: disable=protected-access\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1322, in _call_flat\n",
            "    return self._inference_function.call_preflattened(args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 216, in call_preflattened\n",
            "    flat_outputs = self.call_flat(*args)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 251, in call_flat\n",
            "    outputs = self._bound_context.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\", line 1688, in call_function\n",
            "    outputs = execute.execute(\n",
            "              ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1718, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "                  ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 223, in findsource\n",
            "    pat = re.compile(r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/re/__init__.py\", line 228, in compile\n",
            "    return _compile(pattern, flags)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/re/__init__.py\", line 307, in _compile\n",
            "    p = _compiler.compile(pattern, flags)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/re/_compiler.py\", line 750, in compile\n",
            "    p = _parser.parse(p, flags)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/re/_parser.py\", line 979, in parse\n",
            "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/re/_parser.py\", line 460, in _parse_sub\n",
            "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3057412656.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1236\u001b[0m     \u001b[0;31m# Run the enhanced multi-model pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomparison\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_model_can_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3057412656.py\u001b[0m in \u001b[0;36mmulti_model_can_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;31m# 3. Train and evaluate all models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_multi_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3057412656.py\u001b[0m in \u001b[0;36mtrain_and_evaluate_multi_models\u001b[0;34m(X_train, X_test, timesteps_val, n_features_val)\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" Starting hyperparameter search for {model_name}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             tuner.search(\n\u001b[0m\u001b[1;32m    795\u001b[0m                 \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         if self.oracle.get_trial(trial.trial_id).metrics.exists(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2872cbf0",
        "outputId": "a12493bc-0c16-42bf-f130-250ff573c670"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_output_dir = '/content/drive/MyDrive/CAN_Autoencoder_Results'\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "os.makedirs(drive_output_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Google Drive mounted and destination folder created at: {drive_output_dir}\")\n",
        "\n",
        "files_to_copy = [\n",
        "    'multi_model_performance_comparison.csv',\n",
        "    'can_multi_model_comparison_report.txt'\n",
        "]\n",
        "\n",
        "# Add model-specific files based on the results dictionary\n",
        "if 'results' in locals() and results is not None:\n",
        "    for model_name in results.keys():\n",
        "        files_to_copy.append(f'can_{model_name.lower()}_autoencoder.h5')\n",
        "        files_to_copy.append(f'can_{model_name.lower()}_hyperparameters.csv')\n",
        "        files_to_copy.append(f'can_{model_name.lower()}_training_history.csv')\n",
        "\n",
        "    # Add the best model file if it exists\n",
        "    if 'best_model' in locals() and best_model is not None and 'best_model_name' in locals() and best_model_name is not None:\n",
        "         best_model_filename = f'best_model_{best_model_name.lower()}_autoencoder.h5'\n",
        "         if os.path.exists(best_model_filename):\n",
        "             files_to_copy.append(best_model_filename)\n",
        "\n",
        "\n",
        "# Copy each file to Google Drive\n",
        "print(\"\\nCopying files to Google Drive...\")\n",
        "copied_files = []\n",
        "for file_name in files_to_copy:\n",
        "    source_path = os.path.join('.', file_name) # Look for files in the current directory\n",
        "    destination_path = os.path.join(drive_output_dir, file_name)\n",
        "    if os.path.exists(source_path):\n",
        "        try:\n",
        "            shutil.copy(source_path, destination_path)\n",
        "            copied_files.append(file_name)\n",
        "            print(f\" Copied: {file_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\" Error copying {file_name}: {e}\")\n",
        "    else:\n",
        "        print(f\" Skipped: {file_name} (not found)\")\n",
        "\n",
        "print(f\"\\nFinished copying {len(copied_files)} files to Google Drive.\")\n",
        "if len(copied_files) < len(files_to_copy):\n",
        "    print(\"Note: Some expected files were not found or could not be copied.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted and destination folder created at: /content/drive/MyDrive/CAN_Autoencoder_Results\n",
            "\n",
            "Copying files to Google Drive...\n",
            " Copied: multi_model_performance_comparison.csv\n",
            " Copied: can_multi_model_comparison_report.txt\n",
            " Copied: can_lstm_autoencoder.h5\n",
            " Skipped: can_lstm_hyperparameters.csv (not found)\n",
            " Copied: can_lstm_training_history.csv\n",
            " Copied: can_conv1d_autoencoder.h5\n",
            " Skipped: can_conv1d_hyperparameters.csv (not found)\n",
            " Copied: can_conv1d_training_history.csv\n",
            " Copied: can_gru_autoencoder.h5\n",
            " Skipped: can_gru_hyperparameters.csv (not found)\n",
            " Copied: can_gru_training_history.csv\n",
            " Copied: can_hybrid_autoencoder.h5\n",
            " Skipped: can_hybrid_hyperparameters.csv (not found)\n",
            " Copied: can_hybrid_training_history.csv\n",
            " Copied: can_attention_autoencoder.h5\n",
            " Skipped: can_attention_hyperparameters.csv (not found)\n",
            " Copied: can_attention_training_history.csv\n",
            "\n",
            "Finished copying 12 files to Google Drive.\n",
            "Note: Some expected files were not found or could not be copied.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}